{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston House Price DataSet\n",
    "\n",
    "### Eimear Butler G00364802 Machine Learning and Statistics (Dr. Ian McLoughlan)\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The well known Boston DataSet is routinely used as a teaching aid. It is a natural dataset fist published in 1978 containing US census data concerning houses in various suburbs around the city of Boston. Each of the 506 data entries (suburbs) has 13 characteristics documented. All data entries, with the exception of the column titles, are numerical. \n",
    "\n",
    "In this Jupyter Notebook, I aim to \n",
    "-  **describe** the DataSet in more detail\n",
    "-  **analyse** whether there is a significant difference in median house prices between houses that are along the Charles river and those that arenâ€™t\n",
    "-  create a neural network that can **predict** the median house price based on the other variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The packages we will be using are as follows:\n",
    "\n",
    "import numpy as np      #useful throughout the notebook for analysis of data\n",
    "import pandas as pd     #useful throughout the notebook for analysis & presentation of data in array form\n",
    "import scipy.stats as ss #useful throughout the notebook for analysis of data\n",
    "import matplotlib.pyplot as plt    #useful throughout the notebook for visualisation of data\n",
    "import seaborn as sns              #useful throughout the notebook for visualisation of data\n",
    "from tensorflow.python import keras     #useful to develop neural network and make estimations\n",
    "#print(keras.__version__)\n",
    "import keras as kr                      #useful to develop neural network and make estimations \n",
    "from sklearn import preprocessing as pre       #useful to prepare our data before feeding it into a neural network\n",
    "from sklearn.metrics import r2_score    #useful to assess accuracy of estimations when working with linear data\n",
    "import math as math       #for any extra mathematical functions throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just sets the default plot size to be bigger.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The Boston Dataset it is available directly from sklearn \n",
    "from sklearn.datasets import load_boston          #https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#or anaconda which I will use here 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv' \n",
    "\n",
    "df0 = pd.read_csv('C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv') #or boston['data'] could be used\n",
    "df1 = pd.DataFrame(df0.values[1:], columns=df0.iloc[0]) # move the 'feature_names' title row to become the Pandas df title   #source: https://stackoverflow.com/questions/26147180/convert-row-to-column-header-for-pandas-dataframe\n",
    "df = df1.astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.isnull().sum()    # will also check if there are any NaN's in each column, here there are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#round(df.describe(), 3) #describe is a useful function to review the overall parameters of yoru dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `boston['feature_names'])` we confirm the titles for each of the columns.\n",
    "\n",
    "Furthermore, using `boston['DESCR']` gives us the description of each feature in each suburb. \n",
    "\n",
    "A summary is presented below:\n",
    "\n",
    "\n",
    "1. **CRIM**    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- per capita crime rate by town \n",
    "+ **ZN**       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- proportion of residential land zoned for lots over 25,000 sq.ft.       \n",
    "+ **INDUS**    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- proportion of non-retail business acres per town\n",
    "+ **CHAS**     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)      \n",
    "+ **NOX**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- nitric oxides concentration (parts per 10 million)\n",
    "+ **RM**       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- average number of rooms per dwelling\n",
    "+ **AGE**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- proportion of owner-occupied units built prior to 1940       \n",
    "+ **DIS**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- weighted distances to five Boston employment centres        \n",
    "+ **RAD**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- index of accessibility to radial highways \n",
    "+ **TAX**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- full-value property-tax rate per \\$10,000      \n",
    "+ **PTRATIO**  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- pupil-teacher ratio by town   \n",
    "+ **B**        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- 1000(Bk - 0.63)^2 where Bk is the proportion of [people of African American descent] by town       \n",
    "+ **LSTAT**    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- % lower status of the population\n",
    "+ **MEDV**     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Median value of owner-occupied homes in $1000's  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The CHAS & MEDV Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the data presented above, I can see that the key features we are interested in during the first part of this review are displayed. The MEDV (the mean value of the homes in the suburb) and CHAS (the data documenting which suburbs are by the Charles river) will be particularly relevant to us.\n",
    "\n",
    "\n",
    "The CHAS is a binary data set meaning that there is only 2 possible entries, '1' if the suburb is by the river and '0'' if it is not. This immediately presents an opportunity for us to split the data into 2 groups to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_chas = df1.iloc[:,3].str.contains('1', regex=False)         #segragate the CHAS entries \n",
    "df_chas1 = pd.concat([df['MEDV'], df_chas], axis=1, sort=False)  #add them to the MEDV data column\n",
    "df_chas1\n",
    "\n",
    "chas_t = ((df_chas1[df_chas1['CHAS'] == True]).drop(['CHAS'], axis=1)) #identify those that are beside the river\n",
    "chas_f = (df_chas1[df_chas1['CHAS'] == False]).drop(['CHAS'], axis=1) #and those that are not\n",
    "\n",
    "print('The number of in scope suburbs by the Charles River are', len(chas_t), 'from 506. This set is now called chas_t')\n",
    "\n",
    "print('The maximum price from the sample of houses close to the river is $',chas_t['MEDV'].max(),'and minimum $', chas_t['MEDV'].min())\n",
    "print('The mean price is $', chas_t['MEDV'].mean(), 'while the median is $', chas_t['MEDV'].median())\n",
    "print('')\n",
    "print('The number of in scope suburbs NOT by the Charles River are', len(chas_f),  'from 506. This set is now called chas_f')\n",
    "print('The maximum price from the sample of houses away from the river is $',chas_f['MEDV'].max(),'and minimum $', chas_f['MEDV'].min())\n",
    "print ('The mean price is $', round(chas_f['MEDV'].mean(),2), 'while the median is $', chas_f['MEDV'].median())\n",
    "\n",
    "print ('NOTE: the MEDV has clearly been capped at $50k for the purpose of this dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "\n",
    "features = ['CHAS'] #https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n",
    "target = df['MEDV']\n",
    "\n",
    "for i, col in enumerate(features):  #create a plot \n",
    "    plt.subplot(1, len(features) , i+1)\n",
    "    x = target\n",
    "    y = df[col]\n",
    "    plt.scatter(x, y, marker='o')\n",
    "    plt.title(col)\n",
    "    plt.ylabel('CHAS Yes = 1, No = 0')\n",
    "    plt.yticks(np.arange(0, 1.1, 1.0))\n",
    "    plt.xlabel('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the spread of the average prices(MEDV) across the 2 data sets (beside the Charles River (1) or not (0)), confirms that the average price is not very much higher however the min price of 13.4 is geatly influencing the \"beside the Charles River\" data set. \n",
    "\n",
    "\n",
    "Viewing the same data in histograms again shows the difference between the 2 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(chas_t, color='b', axlabel='MEDV', bins=10) # houses close to the river are in blue\n",
    "sns.distplot(chas_f, color='r') # houses farther from the river are in red\n",
    "#sns.distplot(df['MEDV'], color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss.ttest_ind(chas_t, chas_f) #source: Machine Learning Lectures, Dr. Ian McLoughlan 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ttest between the 2 groups asks the question, what is the probability that the 2 sample groups are from the same overall population? In this case the p value is extremely low and indicates that the the 2 groups are indeed different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(18,8))\n",
    "correlation_matrix = df.corr().round(2) # Source: https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n",
    "sns.heatmap(data=correlation_matrix, annot=True) # annot = True to print the values inside the square\n",
    "\n",
    "# THe following additional code is added to resolve a known bug while displaying the heatmap source: https://github.com/mwaskom/seaborn/issues/1773\n",
    "b, t = plt.ylim() # discover the values for bottom and top \n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there  is a strong positive correlation between the two variables. When it is close to -1, the variables have a strong negative correlation.\" \n",
    "\n",
    "Source: https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n",
    "\n",
    "As per the above quote, very dark (close to -1) show a strong negative correlation or very light areas (close to 1) a strong positive correlation.  For example, the strongest positive correlation between RAD and TAX is seen with a score of 0.91. This means the suburbs with the most accessibility to radial highways are more likely to pay the full-value property-tax rate per $10,000.\n",
    "\n",
    "In the case of CHAS and MEDV, a score of 0.18 is assigned indicating that there is no significant correlation between the 2 characteristics. Keeping in mind that only 35 houses of 506 are by the Charles River, I feel this test was worth running if even just to discount that there is no obvious correlation we are overlooking. It could be noted that CHAS does not have any very strong correlations with the other characteristics either and MEDV is in fact the strongest.\n",
    "\n",
    "This heatmap shows us other interesting correlations too which will move onto next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Broader Analysis of Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"RM has a strong positive correlation with MEDV (0.7) where as LSTAT has a high negative correlation with MEDV(-0.74).\"\n",
    "\n",
    "As a principle, we shouldnt pick both of these features for machine learning, just one, same with AGE and DIS\n",
    "\n",
    "â€˜RMâ€™, â€˜LSTATâ€™(OUT), â€˜PTRATIOâ€™ and â€˜MEDVâ€™. are the most relevant \n",
    "\n",
    "RM                - average number of rooms per dwelling\n",
    "PTRATIO      - pupil-teacher ratio by town\n",
    "LSTAT          - % lower status of the population\n",
    "MEDV          - Median value of owner-occupied homes in $1000's\n",
    "\n",
    "cols = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV'] source: https://subscription.packtpub.com/book/programming/9781789804744/1/ch01lvl1sec11/our-first-analysis-the-boston-housing-dataset\n",
    "\n",
    "df[cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(df[['MEDV', 'RM','LSTAT','AGE', 'PTRATIO', 'RAD', 'TAX']], hue='RAD') #create grid of plots where each variable is compared and colours determine the property type \n",
    "sns.pairplot(df[['MEDV', 'RM','LSTAT','AGE', 'PTRATIO', 'RAD', 'TAX']]) #create grid of plots where each variable is compared and colours determine the property type \n",
    "\n",
    "#Source:https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms \n",
    "MEDV and RM are closest to normal distribution - do somethign with MEDV normal distribution??https://subscription.packtpub.com/book/programming/9781789804744/1/ch01lvl1sec11/our-first-analysis-the-boston-housing-dataset\n",
    "\n",
    "Odd 24 RAD is seen and tax at 700 (potentially capped) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section X: Further Investigation LSTAT vs MDEV and RM vs MDEV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trialing LSTAT & RM Lines for BestFit \n",
    "\n",
    "First lets use the numpy polyfit to find thebest fit striaght line for these data sets and visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_dfRM = df['RM']\n",
    "y_dfRM = df['MEDV']\n",
    "plt.subplot(1, 2, 2)            #source:https://stackoverflow.com/questions/42818361/how-to-make-two-plots-side-by-side-using-python\n",
    "plt.plot(x_dfRM,y_dfRM, 'bo',  label='LSTAT Original Data Points')   #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "z_dfRM= np.polyfit(x_dfRM, y_dfRM, 1)\n",
    "pRM = np.poly1d(z_dfRM) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "\n",
    "plt.plot(x_dfRM,pRM(x_dfRM),\"g--\", label='PolyFit Best Straight Line RM') #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "#where the resuting m and c are values in the equation of a straight line (y=mx+c)\n",
    "#pRM  # Y = 9.10210898 x + (-34.67062078)\n",
    "\n",
    "############################################\n",
    "\n",
    "x_df2 = df['LSTAT']\n",
    "y_df2 = df['MEDV']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_df2,y_df2, 'bo', label='LSTAT Original Data Points')   #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "z_df2= np.polyfit(x_df2, y_df2, 1)\n",
    "p2 = np.poly1d(z_df2) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "\n",
    "plt.plot(x_df2,p2(x_df2),\"g--\", label='PolyFit Best Straight Line LSTAT') #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "\n",
    "#where the resuting m and c are values in the equation of a straight line (y=mx+c)\n",
    "#p2  # Y = -0.95004935x + 34.55384088\n",
    "\n",
    "#############################################\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or in seaborn with 95% confidence intervals https://subscription.packtpub.com/book/programming/9781789804744/1/ch01lvl1sec11/our-first-analysis-the-boston-housing-dataset\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "sns.regplot('RM', 'MEDV', df, ax=ax[0], ci=95, scatter_kws={'alpha': 0.4})\n",
    "sns.regplot('LSTAT', 'MEDV', df, ax=ax[1], ci=95, scatter_kws={'alpha': 0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The standard error of the mean, also called the standard deviation of the mean, is a method used to estimate the standard deviation of a sampling distribution.\" Source: https://explorable.com/standard-error-of-the-mean\n",
    "\n",
    "![alt text](https://explorable.com/images/standard-error-of-the-mean.jpg \"Title\")\n",
    "\n",
    "* ÏƒM = standard error of the mean\n",
    "\n",
    "* Ïƒ = the standard deviation of the original distribution\n",
    "\n",
    "* N = the sample size\n",
    "\n",
    "\n",
    "\n",
    "It gives us a quantifiable method to determine how good a fit our \"predictive\", least squares method lines are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#function to calculate the performance score between true and predicted values based on the metric chosen. Source: https://www.ritchieng.com/machine-learning-project-boston-home-prices/\n",
    "def performance_metric(y_true, y_predict):\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    return score\n",
    "\n",
    "#Calculate the performance of this model  \n",
    "score = performance_metric(y_df2, p2(x_df2))\n",
    "\n",
    "#Calculate the performance of this model  \n",
    "score2 = performance_metric(y_dfRM, pRM(x_dfRM))\n",
    "\n",
    "##############################\n",
    "\n",
    "from sklearn.metrics import mean_squared_error #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
    "\n",
    "def get_mse(df, feature, y, y_pred, target='MEDV'):\n",
    "    # Get x, y to model\n",
    "    y = df[target].values\n",
    "    x = df[feature].values.reshape(-1,1)\n",
    "\n",
    "    error = mean_squared_error(y, y_pred)\n",
    "    print('Mean Squared Error of', feature, ' = {:.2f}'.format(error)) \n",
    "\n",
    "print (\"LSTAT polyfit line has a coefficient of determination, R^2, of {:.3f}.\".format(score), \"or\", round(score*100, 2) , \"% accuracy\")\n",
    "print (\"and... \")\n",
    "get_mse(df, 'LSTAT', x_df2, (p2(x_df2)))\n",
    "print (\" \")    \n",
    "print (\"RM polyfit line has a coefficient of determination, R^2, of {:.3f}.\".format(score2), \"or\", round(score2*100, 2) , \"% accuracy\")\n",
    "print (\"and... \")\n",
    "get_mse(df, 'RM', x_dfRM, (pRM(x_dfRM)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTAT has a higher accuracy and lower error and so is the data I shall progress with.\n",
    "\n",
    "However, when you look more closely to the LSTAT data, you notice that it in fact could get a better fit ( and lower Mean Squared Error) if using a curve rather than a straight line. Keeping in mind our observation that the MEDV data most likely was capped at $50k due to the number at this figure, applying a curved line also seems worth a try.\n",
    "\n",
    "I want to try that before I begin modeling with the neural network.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b, c):    #Source: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "    return a * np.exp(-b * x) + c     #Still uses least squares principle but is non linear\n",
    "\n",
    "xdata = df['LSTAT']\n",
    "ydata = df['MEDV']\n",
    "plt.plot(xdata, ydata, 'ko', label='data')\n",
    "\n",
    "#Fit for the parameters a, b, c of the function func:\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata)\n",
    "popt\n",
    "\n",
    "plt.plot(xdata, func(xdata, *popt), 'r-',\n",
    "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
    "\n",
    "curve_1_y_data = func(xdata, *popt)\n",
    "\n",
    "MSE1_curve1 = get_mse(df, 'LSTAT', xdata, func(xdata, *popt))\n",
    "\n",
    "#Constrain the optimization to the region of 0 <= a <= 50, 0 <= b <= 1 and 0 <= c <= 0.5:\n",
    "\n",
    "popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [55., 1., 1.0]))\n",
    "popt\n",
    "\n",
    "plt.plot(xdata, func(xdata, *popt), 'g--',\n",
    "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
    "curve_2_y_data = func(xdata, *popt)\n",
    "\n",
    "MSE1_curve2 = get_mse(df, 'LSTAT', xdata, func(xdata, *popt))\n",
    "\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the MSE reduces to approx. 28 with the red curve so really this is really worth keeping in mind as we begin to model with the neural network. \n",
    "\n",
    "SIDE-NOTE: this makes sense as when doing some training with Keras (where the LOSS is selected as MEAN sQUARED ERROR), BEFORE normalisation, I could only get down to a loss of about 27 (26.x, occasionally) for the LSTAT data. I was graphing the output each time and could see that the model was getting less error with \"elu\"/\"relu\" activations which were allowing more curvature in the predictive \"line\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = pre.StandardScaler().fit(df) #source:https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaler.transform(df))\n",
    "df_scaled.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "#df_scaled.sort_index() # = df_scaled1.sort_values(by=['PTRATIO'])\n",
    "\n",
    "xscale_train_data = df_scaled.iloc[0:404, 0:13] \n",
    "noscale_train_targets = df.iloc[0:404, 13:14] \n",
    "xscale_test_data = df_scaled.iloc[404:507, 0:13]\n",
    "noscale_test_targets = df.iloc[404:507, 13:14]\n",
    "print(xscale_train_data.shape, xscale_train_targets.shape, xscale_test_data.shape, xscale_test_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inputs = preprocessing.scale(df[['LSTAT']]).ravel() #se np.ravel (for a 1D view), https://stackoverflow.com/questions/13730468/from-nd-to-1d-arrays\n",
    "#output = preprocessing.scale(df['MEDV'])\n",
    "\n",
    "inputs = xscale_train_data['LSTAT'].ravel()\n",
    "output = noscale_train_targets['MEDV'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create a new neural network.\n",
    "n = kr.models.Sequential()\n",
    "\n",
    "# Add neurons\n",
    "n.add(kr.layers.Dense(65, input_dim=1, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(100, activation=\"tanh\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(70, activation=\"sigmoid\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(45, activation=\"elu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(45, activation=\"selu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(10, activation=\"softmax\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(60, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.closen.add(kr.layers.Dense(45, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(60, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.closen.add(kr.layers.Dense(45, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(60, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.closen.add(kr.layers.Dense(45, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "n.add(kr.layers.Dense(1, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "\n",
    "# Compile the model.\n",
    "n.compile(loss=\"mean_squared_error\", optimizer=\"Adadelta\") # through testing the optomisers from the https://keras.io/optimizers/ website, the best was found to be Adadelta\n",
    "\n",
    "# Train the model.\n",
    "n.fit(inputs, output, epochs=25,batch_size=10)\n",
    "\n",
    "# Run each x value through the neural network.\n",
    "p_2 = n.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#p_2.T #estimated MEDV values using model\n",
    "#output.as_matrix() #actual MEDV values\n",
    "\n",
    "#calculate mean squared error\n",
    "#np.sqrt(np.sum((p_2.T - output.as_matrix())**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the values. #AdaDelta\n",
    "plt.plot(inputs, output, 'co', label='Original Data Points')\n",
    "#x_curve = np.linspace(4,9,100)\n",
    "#y_curve = x_curve**2 + 9.10210898*r13 -34.67062078\n",
    "#plt.plot(r13,t13, 'b-', label='Regression on Original Data Points')\n",
    "\n",
    "fit = np.poly1d(np.polyfit(inputs, output, 1))\n",
    "plt.plot(inputs, fit(inputs),'k-', label='Numpy Bestfit Straight Line')\n",
    "\n",
    "plt.plot(inputs, p_2, 'r-', label='Prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict:\n",
    "i=0\n",
    "results=[]\n",
    "\n",
    "while i<len(xscale_test_data):\n",
    "    q_5 = n.predict([xscale_test_data['LSTAT'].iloc[i]])\n",
    "#    print(q_5.type)\n",
    "    results.append([q_5])\n",
    "    i=i+1\n",
    "    \n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOTE: can only run this cell once...\n",
    "#if error flagged, just create results array again by running the above cell\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.iloc[:,0] = results.iloc[:,0].str.get(0) # needs to be used twice to eliminate brackets from DF however will not work in IF/WHILE loop!\n",
    "results.iloc[:,0] = results.iloc[:,0].str.get(0)\n",
    "\n",
    "results.columns=['Xscaled_Results']\n",
    "results1 = results.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xscale_test_targets  =  the expected results we know from 102 entries in the original dataset\n",
    "# LSTAT Estimation  =  the estimated results the neural network has produced\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "xscale_test_targets1 = xscale_test_targets\n",
    "xscale_test_targets1['MEDV Estimation'] = results1\n",
    "#xscale_test_targets['difference at scaled level'] = (xscale_test_targets['MEDV'])**2 - (xscale_test_targets['LSTAT Estimation'])**2\n",
    "round(xscale_test_targets1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n.evaluate(xscale_test_targets1['MEDV Estimation'],xscale_test_targets1['MEDV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict:\n",
    "test_set1 = pd.DataFrame([-1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5])\n",
    "test_set = test_set1.as_matrix()\n",
    "i=0\n",
    "test_results=[]\n",
    "\n",
    "while i<len(test_set):\n",
    "    q_5 = n.predict([test_set[i]])\n",
    "#    print(q_5.type)\n",
    "    test_results.append([q_5, 0])\n",
    "    i=i+1\n",
    "\n",
    "test_results2 = pd.DataFrame(test_results)\n",
    "test_results2.iloc[:,0] = test_results2.iloc[:,0].str.get(0)\n",
    "test_results2.iloc[:,0] = test_results2.iloc[:,0].str.get(0)\n",
    "test_results2[\"inputs\"] = test_set\n",
    "test_results2.drop([1], axis=1)\n",
    "test_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_results2[0], test_results2['inputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit = np.poly1d(np.polyfit(xdata, ydata, 5))\n",
    "plt.plot(xdata, fit(xdata),'k-', label='Numpy Bestfit LSTAT')\n",
    "z_df= np.polyfit(inputs2, output2, 1)\n",
    "p3 = np.poly1d(z_df) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "plt.plot(inputs2,p3(inputs2),\"g--\", label='Numpy Bestfit PTRATIO') #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "\n",
    "plt.show()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_copy3 = df[['MEDV', 'PTRATIO', 'LSTAT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs2 = xscale_train_data['PTRATIO'].ravel()\n",
    "output2 = noscale_train_targets['MEDV'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new neural network.\n",
    "o = kr.models.Sequential()\n",
    "\n",
    "# Add neurons\n",
    "o.add(kr.layers.Dense(50, input_dim=1, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "o.add(kr.layers.Dense(20, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "o.add(kr.layers.Dense(1, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "\n",
    "\n",
    "# Compile the model.\n",
    "o.compile(loss=\"mean_squared_error\", optimizer=\"Adadelta\") # through testing the optomisers from the https://keras.io/optimizers/ website, the best was found to be Adadelta\n",
    "\n",
    "# Train the model.\n",
    "o.fit(inputs2, output2, epochs=20,batch_size=10)\n",
    "\n",
    "# Run each x value through the neural network.\n",
    "p_3 = o.predict(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the values. #AdaDelta\n",
    "#plt.plot(inputs, output, 'co', label='Original Data Points')\n",
    "plt.plot(inputs2,output2, 'bo', label='Original Data')\n",
    "plt.plot(inputs2, p_3, 'r-', label='Prediction')\n",
    "\n",
    "z_df2= np.polyfit(inputs2, output2, 1)\n",
    "p3 = np.poly1d(z_df2) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "plt.plot(inputs2,p3(inputs2),\"g--\", label='Numpy Bestfit') #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#where the resuting m and c are values in the equation of a straight line (y=mx+c)\n",
    "p3  # Y = -4.00198154 x + 23.38138783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#function to calculate the performance score between true and predicted values based on the metric chosen. Source: https://www.ritchieng.com/machine-learning-project-boston-home-prices/\n",
    "def performance_metric(y_true, y_predict):\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    return score\n",
    "\n",
    "#Calculate the performance of this model  \n",
    "score = performance_metric(output2, p3(inputs2))\n",
    "print (\"Line (green) has a coefficient of determination, R^2, of {:.3f}.\".format(score), \"or\", round(score*100, 2) , \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================##############################======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs3 = xscale_train_data[['PTRATIO', 'LSTAT']]\n",
    "output3 = noscale_train_targets['MEDV'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create a new neural network.\n",
    "q = kr.models.Sequential()\n",
    "\n",
    "# Add neurons\n",
    "q.add(kr.layers.Dense(20, input_dim=2, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "q.add(kr.layers.Dense(45, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "q.add(kr.layers.Dense(60, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "q.add(kr.layers.Dense(60, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "q.add(kr.layers.Dense(20, activation=\"relu\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "q.add(kr.layers.Dense(1, activation=\"linear\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "\n",
    "# Compile the model.\n",
    "q.compile(loss=\"mean_squared_error\", optimizer=\"Adadelta\") # through testing the optomisers from the https://keras.io/optimizers/ website, the best was found to be Adadelta\n",
    "\n",
    "# Train the model.\n",
    "q.fit(inputs3, output3, epochs=2000,batch_size=10)\n",
    "\n",
    "# Run each x value through the neural network.\n",
    "q_3 = q.predict(inputs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.evaluate(Results_df_2['Estimation Results1'],Results_df_2['MEDV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate the performance of this model but only if viewing it as a linear model \n",
    "#this metric should be taken with a pinch of salt as although both are similar we have seen LSTAT likes to curve a lot more than the model for PTRATIO\n",
    "score = performance_metric(output3, q_3)\n",
    "print (\"Line (green) has a coefficient of determination, R^2, of {:.3f}.\".format(score), \"or\", round(score*100, 2) , \"% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict:\n",
    "i=0\n",
    "results2=[]\n",
    "\n",
    "while i<len(xscale_test_data['LSTAT']):\n",
    "    q_5 = q.predict(pd.DataFrame(xscale_test_data[['PTRATIO', 'LSTAT']].iloc[i]).T)\n",
    "#    print(q_5.type)\n",
    "    results2.append([q_5,0])\n",
    "    i=i+1\n",
    "    \n",
    "#print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOTE: can only run this cell once...\n",
    "#if error flagged, just create results array again by running the above cell\n",
    "\n",
    "results2 = pd.DataFrame(results2)\n",
    "results2.iloc[:,0] = results2.iloc[:,0].str.get(0) # needs to be used twice to eliminate brackets from DF however will not work in IF/WHILE loop!\n",
    "results2.iloc[:,0] = results2.iloc[:,0].str.get(0)\n",
    "\n",
    "results2.columns=['Xscaled_Results', 'Extra']\n",
    "results2.drop(columns=['Extra'])\n",
    "#results2a = results2.values.ravel()\n",
    "#results2b = pd.DataFrame(results2a)#.drop(columns=['Extra']) \n",
    "results2b = pd.DataFrame(results2).iloc[0:102, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xscale_test_targets  =  the expected results we know from 102 entries in the original dataset\n",
    "# LSTAT Estimation  =  the estimated results the neural network has produced\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "xscale_test_targets.drop(columns=['MEDV Estimation'], axis=1)\n",
    "Results_df_2 = xscale_test_targets\n",
    "Results_df_2['Estimation Results1'] = results2b.values.ravel()\n",
    "\n",
    "#xscale_test_targets['difference at scaled level'] = (xscale_test_targets['MEDV'])**2 - (xscale_test_targets['LSTAT Estimation'])**2\n",
    "round(Results_df_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs3['PTRATIO'], output3, 'go')\n",
    "plt.plot(inputs3['LSTAT'], output3, 'yo')\n",
    "\n",
    "q3 = np.poly1d(np.polyfit(inputs3['PTRATIO'], output2, 1)) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "plt.plot(inputs3['PTRATIO'],q3(inputs3['PTRATIO']),\"r-\", label='Numpy Bestfit PTRATIO') #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "\n",
    "q3a = np.poly1d(np.polyfit(inputs3['LSTAT'], output3, 5))\n",
    "plt.plot(inputs3['LSTAT'], q3a(inputs3['LSTAT']),'k-', label='Numpy Bestfit LSTAT')\n",
    "\n",
    "#plt.plot(xscale_test_data, noscale_test_targets, 'ro', marker='.')\n",
    "\n",
    "#plt.plot(pd.DataFrame(scaler.transform(Results_df_2['Estimation Results1'])), noscale_test_targets, 'bo', marker='.')\n",
    "\n",
    "\n",
    "plt.plot(inputs3, q_3, 'co', label='Prediction')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_test = [[18,2.5],[15,2],[35,6]] \n",
    "q_5a = pd.DataFrame(input_test)\n",
    "q_5 = q.predict(q_5a)\n",
    "q_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "### What if the size of the property is a factor to consider?\n",
    "MEDV is a curious feature as it appears to be a total value for the property not taking into account for the size of it. After location, the size or quality of the building are surely the biggest factors influencing price. \n",
    "\n",
    "As we do have a column that is an inidicator of size, the average number of rooms per dwelling (RM), I will divide the total price by the average number of rooms to get an indicative square metre price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sm = df.filter(items=['CHAS', 'RM','MEDV'])\n",
    "df_sm2 =  df_sm.MEDV / df_sm.RM \n",
    "df_sm3 = pd.concat([df_sm, df_sm2], axis=1, sort=False).drop(['RM', 'MEDV'], axis=1)\n",
    "df_sm3.columns = ('CHAS', 'SME') #SME for Square Metre estimate\n",
    "#df_sm3\n",
    "\n",
    "sme_t = (df_sm3[df_sm3['CHAS'] == 0].drop(['CHAS'], axis=1))\n",
    "sme_f = (df_sm3[df_sm3['CHAS'] == 1].drop(['CHAS'], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ss.ttest_ind(sme_t, sme_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P value is still not close enough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(sme_t, color='k', axlabel='MEDV') # house MEDV prices close to the river are in b;ack\n",
    "sns.distplot(sme_f, color='g') # house MEDV prices farther from the river are in green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra investigation into RAD and TAX relationship\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Firstly I built a function to split out data and allow me to review for trends before deciding which characteristics to choose for predictions.\n",
    "\n",
    "The function is based on the below principles:\n",
    "\n",
    "That the unique groupigns can be identified easily using .unique(),\n",
    "`df1.RAD.unique()` \n",
    "\n",
    "\n",
    "and each group then separated for further analysis.\n",
    "`df_rad1 = ((df1[df1['RAD'] == \"4\"]))`\n",
    "\n",
    "The first review was for RAD vs TAX based on the correlation we could see in the heat map above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def impact(rating, column_name_string1, column_name_string2, column_name_string_return):\n",
    "        df_rad1 = ((df1[df1[column_name_string1] == str(rating)]))\n",
    "        df_rad11 = pd.concat([df[column_name_string2], df_rad1], axis=1, sort=False)\n",
    "        rad_1y = ((df_rad11[df_rad11[column_name_string1] == True]).drop([column_name_string1], axis=1))\n",
    "        rad_1y1 = rad_1y.values\n",
    "        #len_1x = len(rad_1y)\n",
    "        rad_1x = []\n",
    "        i = 0\n",
    "        while i<len(rad_1y):\n",
    "            i=i+1\n",
    "            rad_1x.append(rating)\n",
    "        return df_rad1[column_name_string_return]\n",
    "    \n",
    "#####################################\n",
    "\n",
    "run1 = impact(1, 'RAD', 'TAX', 'TAX')\n",
    "run2 = impact(2, 'RAD', 'TAX', 'TAX')\n",
    "run3 = impact(3, 'RAD', 'TAX', 'TAX')\n",
    "run4 = impact(4, 'RAD', 'TAX', 'TAX')\n",
    "run5 = impact(5, 'RAD', 'TAX', 'TAX')\n",
    "run6 = impact(6, 'RAD', 'TAX', 'TAX')\n",
    "run7 = impact(7, 'RAD', 'TAX', 'TAX')\n",
    "run8 = impact(8, 'RAD', 'TAX', 'TAX')\n",
    "run9 = impact(24, 'RAD', 'TAX', 'TAX')\n",
    "\n",
    "#Check point:\n",
    "#print(len(rad_impact(24))+len(rad_impact(1))+ len(rad_impact(2))+ len(rad_impact(3))+ len(rad_impact(4))+ len(rad_impact(5))+ len(rad_impact(6))+ len(rad_impact(7))+ len(rad_impact(8)))\n",
    "\n",
    "radx = pd.concat([run1, run2, run3, run4, run5, run6, run7, run8, run9], axis=1)\n",
    "radx.columns=(\"1\", \"2\",\"3\" ,\"4\" ,\"5\" ,\"6\" ,\"7\" ,\"8\", \"24\")\n",
    "\n",
    "run10 = impact(1, 'RAD', 'TAX', 'MEDV')\n",
    "run11 = impact(2, 'RAD', 'TAX', 'MEDV')\n",
    "run12 = impact(3, 'RAD', 'TAX', 'MEDV')\n",
    "run13 = impact(4, 'RAD', 'TAX', 'MEDV')\n",
    "run14 = impact(5, 'RAD', 'TAX', 'MEDV')\n",
    "run15 = impact(6, 'RAD', 'TAX', 'MEDV')\n",
    "run16 = impact(7, 'RAD', 'TAX', 'MEDV')\n",
    "run17 = impact(8, 'RAD', 'TAX', 'MEDV')\n",
    "run18 = impact(24, 'RAD', 'TAX', 'MEDV')\n",
    "\n",
    "#Check point:\n",
    "#print(len(rad_impact(24))+len(rad_impact(1))+ len(rad_impact(2))+ len(rad_impact(3))+ len(rad_impact(4))+ len(rad_impact(5))+ len(rad_impact(6))+ len(rad_impact(7))+ len(rad_impact(8)))\n",
    "\n",
    "radx_2 = pd.concat([run10, run11, run12, run13, run14, run15, run16, run17, run18], axis=1)\n",
    "radx_2.columns=(\"1\", \"2\",\"3\" ,\"4\" ,\"5\" ,\"6\" ,\"7\" ,\"8\", \"24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.catplot(kind=\"box\",data=radx, showmeans=True)\n",
    "ax.set(xlabel='RAD Rating', ylabel='TAX $')\n",
    "\n",
    "ay = sns.catplot(kind=\"box\",data=radx_2, showmeans=True)\n",
    "ay.set(xlabel='RAD Rating', ylabel='MEDV $')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radx1 = radx.astype(float)\n",
    "radx2 = radx1.mean(axis=0, skipna=True)\n",
    "rad_mean = pd.DataFrame(radx2).T\n",
    "rad_mean.columns=(\"1 (mean)\", \"2 (mean)\",\"3 (mean)\" ,\"4 (mean)\" ,\"5 (mean)\" ,\"6 (mean)\" ,\"7 (mean)\" ,\"8 (mean)\", \"24 (mean)\")\n",
    "rx1 = [\"1 (mean TAX)\", \"2 (mean)\",\"3 (mean)\" ,\"4 (mean)\" ,\"5 (mean)\" ,\"6 (mean)\" ,\"7 (mean)\" ,\"8 (mean)\", \"24 (mean)\"]\n",
    "ry1 = rad_mean.iloc[0,:]\n",
    "round(rad_mean, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radx1 = radx_2.astype(float)\n",
    "radx2 = radx1.mean(axis=0, skipna=True)\n",
    "rad_mean = pd.DataFrame(radx2).T\n",
    "rad_mean.columns=(\"1 (mean)\", \"2 (mean)\",\"3 (mean)\" ,\"4 (mean)\" ,\"5 (mean)\" ,\"6 (mean)\" ,\"7 (mean)\" ,\"8 (mean)\", \"24 (mean)\")\n",
    "rx2 = [\"1 (mean MEDV)\", \"2 (mean)\",\"3 (mean)\" ,\"4 (mean)\" ,\"5 (mean)\" ,\"6 (mean)\" ,\"7 (mean)\" ,\"8 (mean)\", \"24 (mean)\"]\n",
    "ry2 = rad_mean.iloc[0,:]\n",
    "round(rad_mean, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(rx1, ry1, '-bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rx2, ry2, '-bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_rad24[['MEDV', 'RM','LSTAT', 'RAD', 'CRIM', 'NOX', 'AGE']], hue='MEDV') #create grid of plots where each variable is compared and colours determine the property type \n",
    "\n",
    "#Source:https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad24 = ((df[df['RAD'] == 24]))  \n",
    "df_rad24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run9 = impact(24, 'RAD', 'TAX', 'TAX')\n",
    "\n",
    "run18 = impact(24, 'RAD', 'TAX', 'MEDV')\n",
    "\n",
    "run19 = impact(24, 'RAD', 'CRIM', 'MEDV')\n",
    "\n",
    "radx_2 = pd.concat([run9, run18], axis=1)\n",
    "\n",
    "\n",
    "CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding if LSTATS or RM should be used - but not both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the most obvious correlation - rooms (indication of size) vs price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy1 = df[['MEDV', 'RM']].copy()\n",
    "df_copy2 = df_copy1.sort_values(by=['RM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_df = df_copy2['RM']\n",
    "y_df = df_copy2['MEDV']\n",
    "plt.plot(x_df,y_df, 'bo')\n",
    "z_df= np.polyfit(x_df, y_df, 1)\n",
    "p = np.poly1d(z_df) #use ployfit function to determine least squares polynomial line fit, where 1 is the Degree of the fitting the polynomial\n",
    "\n",
    "plt.plot(x_df,p(x_df),\"g--\") #Reference: Ian McLoughlin \"Simple Linear Regression with NumPy\" Jupyter Notebook, Semester 2 GMIT \n",
    "plt.show()\n",
    "#where the resuting m and c are values in the equation of a straight line (y=mx+c)\n",
    "p  # Y = 9.10210898 x + (-34.67062078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#function to calculate the performance score between true and predicted values based on the metric chosen. Source: https://www.ritchieng.com/machine-learning-project-boston-home-prices/\n",
    "def performance_metric(y_true, y_predict):\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    return score\n",
    "\n",
    "#Calculate the performance of this model  \n",
    "score = performance_metric(y_df, p2(x_df))\n",
    "print (\"Line (green) has a coefficient of determination, R^2, of {:.3f}.\".format(score), \"or\", round(score*100, 2) , \"% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========Modeling RM========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x = np.linspace(-10.0, 10.0, 506)\n",
    "x = df_copy2 ['RM']\n",
    "y = df_copy2 ['MEDV']\n",
    "\n",
    "\n",
    "# Create a new neural network.\n",
    "m = kr.models.Sequential()\n",
    "\n",
    "# Add neurons.\n",
    "#m.add(kr.layers.Dense(1, input_dim=1, activation=\"linear\"))\n",
    "\n",
    "# Add neurons\n",
    "m.add(kr.layers.Dense(40, input_dim=1, activation=\"tanh\")) #adjusting from 10 to 30,improves loss to 41.x at epoch 45, visually looks v.close\n",
    "m.add(kr.layers.Dense(60, activation=\"tanh\"))\n",
    "m.add(kr.layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model.\n",
    "m.compile(loss=\"mean_squared_error\", optimizer=\"Adadelta\") # through testing the optomisers from the https://keras.io/optimizers/ website, the best was found to be Adadelta\n",
    "# got to 42,x during testing with 60epochs, no change to neuron numbers. \n",
    "\n",
    "# Train the model.\n",
    "m.fit(x, y, epochs=60,batch_size=10)\n",
    "\n",
    "# Run each x value through the neural network.\n",
    "p = m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the values. #AdaDelta\n",
    "plt.plot(x, p, 'r-', label='Prediction')\n",
    "plt.plot(x, y, 'ko', label='Original Data Points')\n",
    "r13 = np.linspace(4,9,100)\n",
    "t13 = 9.10210898*r13 -34.67062078\n",
    "plt.plot(r13,t13, 'b-', label='Regression on Original Data Points')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or in seaborn with 95% confidence intervals https://subscription.packtpub.com/book/programming/9781789804744/1/ch01lvl1sec11/our-first-analysis-the-boston-housing-dataset\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0] = sns.residplot('RM', 'MEDV', df, ax=ax[0],\n",
    "                      scatter_kws={'alpha': 0.4})\n",
    "ax[0].set_ylabel('MDEV residuals $(y-\\hat{y})$')\n",
    "ax[1] = sns.residplot('LSTAT', 'MEDV', df, ax=ax[1],\n",
    "                      scatter_kws={'alpha': 0.4})\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point on these residual plots is the difference between that sample (y) and the linear model prediction ( Å·). Residuals greater than zero are data points that would be underestimated by the model. Likewise, residuals less than zero are data points that would be overestimated by the model.\n",
    "\n",
    "Patterns in these plots can indicate sub optimal modeling. In each preceding case,we see diagonally arranged scatter points in the positive region. These are caused by the $50,000 cap on MEDV. The RM data is clustered nicely around 0, which indicates a good fit. On the other hand, LSTAT appears to be clustered lower than 0.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
